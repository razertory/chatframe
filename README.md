
# ChatFrame

> A cross-platform desktop chatbot that unifies access to multiple LLM providers, supports MCP (Model Context Protocol) servers, and provides built-in retrieval-augmented generation (RAG) for local files. Available for macOS (Apple Silicon & Intel) and Windows (x86_64).

## Overview
ChatFrame delivers a single, polished interface for interacting with language models while giving users full control over their data. A plug-in system for custom tools via MCP and out-of-the-box RAG let you turn any PDF, text, or code file into searchable context—without uploading data to third-party services.

## Quick Start
1. Download the latest release for your OS from [chatframe.co](https://chatframe.co).  
2. Launch the app and open **Providers** to add your API keys. You can validate your configuration by clicking **Verify**.  
3. Click the first **Chat** button one the left to start 
## Supported LLM Providers
- DeepSeek  
- OpenAI 
- Anthropic 
- xAI    
- OpenRouter
- Groq    
- Moonshot
- Cerebras
- Qwen
- GoogleAIStudio
- Zhipu
- Custom Providers(OpenAI compatible)


## MCP (Model Context Protocol) Support
ChatFrame supports SSE, Streamable HTTP, and Stdio MCP servers, similar to [Cursor](https://cursor.com/).  
Add any MCP server to expose its tools at runtime.

### Runtime Environment(STDIO)
Stdio MCP servers require a local runtime environment. For example, to use the Postgres MCP server, 

```json
{
  "postgres": {
    "command": "npx",
    "args": [
      "-y",
      "@modelcontextprotocol/server-postgres",
      "postgresql://localhost/mydb"
    ]
  }
}
```

**Node.js must be installed**.

*Next Step*

DXT (one-click MCP installation, similar to browser extensions) will be introduced in the next major release.

*Why aren’t Node.js and Python bundled?*  
I'd prefer to let users control their own runtime environments. Bundled interpreters can introduce version conflicts and increase the application footprint.

## Projects
Create workspaces, upload local files (PDF, TXT, MD, code), and instantly build vector indexes for retrieval-augmented generation.

## Chat
- Invoke any MCP tool from within a conversation  
- Multimodal input (images and text files)  
- Live **artifacts** – render React components, HTML, SVG, or code snippets in an isolated sandbox that updates in real time

## Pricing
ChatFrame is currently free. Paid plans will be announced in the future.

## Technology
ChatFrame is closed-source and built on [Tauri](https://v2.tauri.app/) and the [Vercel AI SDK](https://ai-sdk.dev/docs/introduction).

## Feature Map
- **Chat**
  - Text chat  
  - Model selection  
  - Tool selection via MCP servers  
  - Artifacts: create interactive content generated by the LLM  
  - Attachments: upload images, PDFs, and text files (PDFs are parsed locally)

- **Projects**  
- **MCP Servers**  
- **Model Providers**  
- **Settings**
  - **App**
    - Appearance: System / Light / Dark  
    - Updates: ChatFrame downloads updates in the background and displays an install button when ready
  - **Shortcuts**
    - New Chat: ⌘N  
    - Toggle Sidebar: ⌘B
  - **Advanced**
    - Proxy URL: sets the `all_proxy` environment variable. Leave blank to disable proxying. When configured, all LLM API requests are routed through the specified proxy.
